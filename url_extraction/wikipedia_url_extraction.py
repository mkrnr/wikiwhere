'''
Created on Jan 12, 2016

@author: Martin Koerner <info@mkoerner.de>
'''

import lxml.etree as ET
import mwparserfromhell
from bz2 import BZ2File
import json

import argparse
from mwparserfromhell.parser import ParserError

# generate help text for arguments
parser = argparse.ArgumentParser(description='Extracts URLS from a list of Wikipedia articles given in the XML dump format and saves it as a JSON file.')
parser.add_argument('input',
                   help='a file path to the output file generated by this program')
parser.add_argument("--output", dest="output", metavar='output path for the JSON file', type=str, required=True)
args = parser.parse_args()

article_link_dictionary = {}


inputfile_path = args.input
outputfile_path = args.output

print "running wikipedia_url_extraction"

def get_namespace(inputfile_path):
    with BZ2File(inputfile_path) as xml_file:
        for line in xml_file:
            # add closing tag or lxml would complain 
            mediawiki_tag = ET.fromstring(line + "</mediawiki>")
            # get xmlns definition
            xmlns = mediawiki_tag.nsmap.get(None)
            break
    return "{" + xmlns + "}"

def extract_urls(text,article_name):
    parsed_text = mwparserfromhell.parse(child_of_revision.text)
    text_tree_split = parsed_text.get_tree().split()
    article_URLs = []
    for element in text_tree_split:
        if element.startswith("http") and ( element.startswith("http://") or element.startswith("https://") ):
            article_URLs.append(element.encode('utf-8'))

    article_link_dictionary[article_name] = article_URLs

# get the correct namespace from the xml file
mediawiki_namespace = get_namespace(inputfile_path) 

page_tag = mediawiki_namespace + "page"
title_tag = mediawiki_namespace + "title"
revision_tag = mediawiki_namespace + "revision"
text_tag = mediawiki_namespace + "text"


with BZ2File(inputfile_path) as xml_file:
    context = ET.iterparse(xml_file, tag = page_tag)

    # iterate over the content of the page tags
    for action, elem in context:
        tree = ET.ElementTree(elem)
        page = tree.getroot()
    
        # iterate over the children of page
        for child_of_page in page:
    
            # get title
            if child_of_page.tag == title_tag:
                article_name = child_of_page.text.encode('utf-8')
    
            else:
                # handle revision case 
                if child_of_page.tag == revision_tag:
    
                    # iterate over children of revision
                    for child_of_revision in child_of_page:
                        if child_of_revision.tag == text_tag: 
    
                            try:
                                # extract external links from the text of the child
                                extract_urls(child_of_revision.text,article_name)
                            except ParserError:
                                print "ParserError for url "+ child_of_revision.text+ " in article "+ article_name
                            
                                        
# write dictionary with key = Wikipedia article and value = URLs to JSON file
with open(outputfile_path, 'w') as f:
    json.dump(article_link_dictionary, f, indent=4, sort_keys=True)
    print "JSON file was stored successfully"
